{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAugmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uY3VsCdRbqVY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY3VsCdRbqVY"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GayT08K0baX9"
      },
      "source": [
        "import os\n",
        "import zipfile"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShSrvCETazco",
        "outputId": "f5747d3b-57a5-4885-8bfc-70ad4f25864c"
      },
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /cats_and_dogs_filtered.zip"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 12:02:35--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.128, 172.253.117.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/cats_and_dogs_filt 100%[===================>]  65.43M  45.2MB/s    in 1.4s    \n",
            "\n",
            "2020-11-26 12:02:37 (45.2 MB/s) - ‘/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64KPme85bUUX"
      },
      "source": [
        "local_zip = '/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thhOuXombU03"
      },
      "source": [
        "base_dir = 'cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PryTzRgPcoJI"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M93smCTNdF1_"
      },
      "source": [
        "batch_size = 20\n",
        "\n",
        "img_height=299\n",
        "img_width=299\n",
        "\n",
        "input_shape = (img_height, img_width, 3,)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2J1Z4PScsZG",
        "outputId": "76a9319c-32cf-4eef-af83-0b5ffc9487d2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=30)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_height, img_width), batch_size=batch_size)\n",
        "valid_generator = valid_datagen.flow_from_directory(validation_dir, target_size=(img_height, img_width), batch_size=batch_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y16vDj6XbvEI"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_TM-V9KcHzv"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (5, 5), activation='relu')) \n",
        "model.add(GlobalAveragePooling2D()) \n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax')) \n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxP5BcoIcZCg"
      },
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmJ81c_bym3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrIrwTxfdNOg",
        "outputId": "48fb3399-e625-47fb-e460-5132d6ea4101"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "epochs = 15\n",
        "patience = 6\n",
        "model_save_path = 'model.hdf5'\n",
        "\n",
        "es = EarlyStopping(patience=patience, verbose=0)\n",
        "mc = ModelCheckpoint(filepath=model_save_path, save_best_only=True, verbose=0)\n",
        "\n",
        "step_size_train = train_generator.n//train_generator.batch_size\n",
        "step_size_valid = valid_generator.n//valid_generator.batch_size\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=50,\n",
        "                    validation_data=valid_generator,\n",
        "                    validation_steps=step_size_valid,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[es, mc])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "50/50 [==============================] - 26s 522ms/step - loss: 0.6732 - accuracy: 0.5760 - val_loss: 0.6857 - val_accuracy: 0.5390\n",
            "Epoch 2/15\n",
            "50/50 [==============================] - 26s 516ms/step - loss: 0.6748 - accuracy: 0.5390 - val_loss: 0.6786 - val_accuracy: 0.5530\n",
            "Epoch 3/15\n",
            "50/50 [==============================] - 25s 509ms/step - loss: 0.6764 - accuracy: 0.5630 - val_loss: 0.6791 - val_accuracy: 0.5880\n",
            "Epoch 4/15\n",
            "50/50 [==============================] - 26s 511ms/step - loss: 0.6655 - accuracy: 0.5810 - val_loss: 0.6795 - val_accuracy: 0.5470\n",
            "Epoch 5/15\n",
            "50/50 [==============================] - 26s 513ms/step - loss: 0.6584 - accuracy: 0.5760 - val_loss: 0.6821 - val_accuracy: 0.5460\n",
            "Epoch 6/15\n",
            "50/50 [==============================] - 26s 514ms/step - loss: 0.6712 - accuracy: 0.5650 - val_loss: 0.6634 - val_accuracy: 0.5700\n",
            "Epoch 7/15\n",
            "50/50 [==============================] - 25s 510ms/step - loss: 0.6675 - accuracy: 0.5780 - val_loss: 0.6592 - val_accuracy: 0.5880\n",
            "Epoch 8/15\n",
            "50/50 [==============================] - 25s 508ms/step - loss: 0.6578 - accuracy: 0.5730 - val_loss: 0.6644 - val_accuracy: 0.5470\n",
            "Epoch 9/15\n",
            "50/50 [==============================] - 26s 511ms/step - loss: 0.6709 - accuracy: 0.5490 - val_loss: 0.6694 - val_accuracy: 0.5490\n",
            "Epoch 10/15\n",
            "50/50 [==============================] - 26s 513ms/step - loss: 0.6766 - accuracy: 0.5600 - val_loss: 0.6659 - val_accuracy: 0.5680\n",
            "Epoch 11/15\n",
            "50/50 [==============================] - 26s 511ms/step - loss: 0.6723 - accuracy: 0.5950 - val_loss: 0.6815 - val_accuracy: 0.5650\n",
            "Epoch 12/15\n",
            "50/50 [==============================] - 26s 512ms/step - loss: 0.6620 - accuracy: 0.5910 - val_loss: 0.7801 - val_accuracy: 0.5140\n",
            "Epoch 13/15\n",
            "50/50 [==============================] - 26s 516ms/step - loss: 0.6659 - accuracy: 0.5450 - val_loss: 0.6551 - val_accuracy: 0.5960\n",
            "Epoch 14/15\n",
            "50/50 [==============================] - 26s 514ms/step - loss: 0.6666 - accuracy: 0.5700 - val_loss: 0.6556 - val_accuracy: 0.5950\n",
            "Epoch 15/15\n",
            "50/50 [==============================] - 26s 515ms/step - loss: 0.6623 - accuracy: 0.5830 - val_loss: 0.6625 - val_accuracy: 0.5880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV648FqHiC7x"
      },
      "source": [
        "# load best model\n",
        "model.load_weights(model_save_path)"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}